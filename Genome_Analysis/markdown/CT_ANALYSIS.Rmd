---
title: "CT_ANALYSIS"
author: "Finn Lobnow"
date: '2022-07-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, include = TRUE, echo = F, message=F}
library(tidyverse)
library(knitr)
library(ggpubr)
library(readr)
library(tibble)
library(leaflet)
library(htmltools)
```

# THE ISOLATES

```{r, isolates, echo=FALSE, message = F}
isolates <- read.csv('https://raw.githubusercontent.com/tlobnow/Crypto/main/Genome_Analysis/markdown/Isolates.csv')
kable(isolates)
```

``` {r, isolates-map, echo = F, message = F, warning = F}

map <- isolates %>% leaflet() %>% addProviderTiles("CartoDB") %>% setView(lat = 52.520007, lng =13.404954, zoom = 2)

tyzzeri <- isolates %>% filter(Spp == "C.tyzzeri")
parvum <- isolates %>% filter(Spp == "C.parvum")
hominis <- isolates %>% filter(Spp == "C.hominis")

mus <- isolates %>% filter(Host == "Mus")
cattle <- isolates %>% filter(Host == "Cattle")
human <- isolates %>% filter(Host == "Human")


map1 <- map %>%
  addPolylines(lat = c(55.0000, 53.6000, 53.51885, 52.8875  , 52.6053, 51.8978, 45.0000), 
               lng = c(10.0000, 11.4563, 12.4464,13.8119 , 13.8756, 13.8103, 10.0000), 
               color = "purple", 
               weight = 10, 
               opacity = 0.4) %>%
  addCircleMarkers(data = cattle,
                   col = "cornflowerblue",
                   opacity = 1,
                   label = ~htmlEscape(Abbr),
                   popup = ~paste("<b>Location:<b>", as.character(Latitude), "<b>,<b>", as.character(Longitude)),
                   radius = 3,
                   group = "Cattle") %>%
  addCircleMarkers(data = human,
                   col = "purple",
                   opacity = 1,
                   label = ~htmlEscape(Abbr),
                   popup = ~paste("<b>Location:<b>", as.character(Latitude), "<b>,<b>", as.character(Longitude)),
                   radius = 3,
                   group = "Human") %>%
  addCircleMarkers(data = mus,
                   col = "yellow",
                   opacity = 1,
                   label = ~htmlEscape(Abbr),
                   popup = ~paste("<b>Location:<b>", as.character(Latitude), "<b>,<b>", as.character(Longitude)),
                   radius = 3,
                   group = "Mus") %>%
  addCircleMarkers(data = parvum,
                   col = "blue",
                   opacity = 1,
                   label = ~htmlEscape(Abbr),
                   popup = ~paste("<b>Location:<b>", as.character(Latitude), "<b>,<b>", as.character(Longitude)),
                   radius = 3,
                   group = "C.parvum") %>%
  addCircleMarkers(data = hominis,
                   col = "orange",
                   opacity = 1,
                   label = ~htmlEscape(Abbr),
                   popup = ~paste("<b>Location:<b>", as.character(Latitude), "<b>,<b>", as.character(Longitude)),
                   radius = 3,
                   group = "C.hominis") %>%
  addCircleMarkers(data = tyzzeri,
                   col = "#EA3C53",
                   opacity = 1,
                   label = ~htmlEscape(Abbr),
                   popup = ~paste("<b>Location:<b>", as.character(Latitude), "<b>,<b>", as.character(Longitude)),
                   radius = 3,
                   group = "C.tyzzeri") %>%
  addLegend("bottomright", 
            colors = c('#FF7F7F', 'blue', 'orange'),
            labels = c('C.tyzzeri',
                       'C.parvum',
                       'C.hominis'),
            title = 'Species',
            opacity = 1) %>%
  addLegend("bottomleft", 
            colors = c('yellow', 'cornflowerblue', 'purple'),
            labels = c('Mus',
                       'Cattle',
                       'Human'),
            title = 'Host',
            opacity = 1) %>%
  addLayersControl(overlayGroups = c('C.tyzzeri', 'C.parvum', 'C.hominis', 'Mus', 'Cattle', 'Human'), 
                   options = layersControlOptions(collapsed = T))
map1
```

# WORKFLOW

## Download data from Sequence Read Archive (SRA)

    prefetch SRR*

## Split files

    fasterq-dump *.sra --split-files

## QC and trimming


--in1 and --in2: specify your files of forward (1) reads and of the reverse (2) reads.

--out1 and --out2: specify the output files for forward and reverse reads that are still Paired.

-l 50: this specifies that if a read is shorter than 50 base pairs after all filters, it should be removed.

-h: specifies name for the html file with plots showing the read quality before and after filtering

#### PolyG tail trimming

This feature removes the polyG tails that arise from lack of signal in NextSeq/NovaSeq technologies. It is enabled for Nextseq/Novaseq data by default, and you can specify -g to enable it for any data, or specify -G to disable it.

#### Removal of adapter sequences

Adapter trimming is enabled by default, but you can disable it with -A. Adapter sequences can be automatically detected for both PE/SE data.

#### Length filter

Reads below the length threshold (e.g. due to adapter removal) are removed. Length filtering is enabled by default. The minimum length requirement is specified with -l.

#### Quality filtering

Quality filtering is enabled by default, but you can disable it with -Q. Currently fastp supports filtering by limiting the number of uncalled (N) bases (-n, Default 5) and the percentage of unqualified bases. To filter reads by its percentage of unqualified bases, two options should be provided:

-q : Quality threshold per base required. Default: 15, which means that a Phred quality score of at least 15 is required

-u : Percent of bases allowed to be below the quality threshold to keep the read (0~100). Default 40 means 40% bases can fail the quality threshold. If more bases fail, the read is removed

    01_fastp.sh
    
    #!/usr/bin/env bash

    INDS=($(for i in /SAN/Ctyzzeri/all/fastq/*_1.fastq.gz; do echo $(basename -a -s _1.fastq.gz $i); done))

    for IND in ${INDS[@]};
    do

        ~/fastp.0.23.1/fastp \
        --in1 /SAN/Ctyzzeri/all/fastq/${IND}_1.fastq.gz \
        --in2 /SAN/Ctyzzeri/all/fastq/${IND}_2.fastq.gz \
        --out1 /SAN/Ctyzzeri/all/results/filteredReads/${IND}.trimmed.R1.fastq.gz \
        --out2 /SAN/Ctyzzeri/all/results/filteredReads/${IND}.trimmed.R2.fastq.gz -l 50 -h /SAN/Ctyzzeri/all/results/qc/filteredReads/${IND}.html &> /SAN/Ctyzzeri/all/results/qc/filteredReads/${IND}.log

    done

## Indexing the Reference Sequence

    bwa index /SAN/Ctyzzeri/all/resources/CryptoDB-57_CtyzzeriUGA55_Genome.fasta
    samtools faidx /SAN/Ctyzzeri/all/resources/*.fasta


## Mapping to the Reference Genome

Using alignment software, we essentially find where in the genome our reads originate from and then once these reads are aligned, we are able to either call variants or construct a consensus sequence for our set of aligned reads.

-M is a standard flag that tells bwa to mark any low quality alignments (i.e. split across long distances) as secondary - we need this for downstream compatability.

-t tells bwa how many threads (cores) on a cluster to use - this effectively determines its speed.

Following these options, we then specify the reference genome, the forward reads and the reverse reads. Finally we write the output of the alignment to a SAM file.

    03_bwa_align.sh
    
    #!/usr/bin/env bash

    REF=/SAN/Ctyzzeri/all/resources/CryptoDB-57_CtyzzeriUGA55_Genome.fasta
    INDS=($(for i in /SAN/Ctyzzeri/all/results/filteredReads/*R1.fastq.gz; do echo $(basename ${i%.R*}); done))

    for IND in ${INDS[@]};
    do

        # declare variables
        FORWARD=/SAN/Ctyzzeri/all/results/filteredReads/${IND}.R1.fastq.gz
        REVERSE=/SAN/Ctyzzeri/all/results/filteredReads/${IND}.R2.fastq.gz
        OUTPUT=/SAN/Ctyzzeri/all/results/align/${IND}_sort.bam

        # align and sort
        echo "Aligning $IND with bwa"
        bwa mem -M -t 16 $REF $FORWARD $REVERSE | \
        samtools view -b | \
        samtools sort -T ${IND} > $OUTPUT

    done
    
Alternatively, this can be run in parallel after specification of all individuals in a list:

    for i in /SAN/Ctyzzeri/all/fastq/*1.fastq.gz; do echo $(basename -a -s _1.fastq.gz $i); done > inds
    
The script can be run using `parallel 'sh parallel_align.sh {}' :::: inds`

    
    parallel_align.sh
    
    #!/usr/bin/env bash

    # align a single individual
    REF=/SAN/Ctyzzeri/all/resources/CryptoDB-57_CtyzzeriUGA55_Genome.fasta

    # declare variables
    IND=$1

    FORWARD=/SAN/Ctyzzeri/all/results/filteredReads/${IND}.trimmed.R1.fastq.gz
    REVERSE=/SAN/Ctyzzeri/all/results/filteredReads/${IND}.trimmed.R2.fastq.gz
    OUTPUT=/SAN/Ctyzzeri/all/results/partest/${IND}_sort.bam

    # align and sort
    echo "Aligning $IND with bwa"
    bwa mem -M -t 16 $REF $FORWARD $REVERSE | \
    samtools view -b | \
    samtools sort -T ${IND} > $OUTPUT
    
    

## Removing Duplicates

Finally, we need to remove duplicate reads from the dataset to avoid PCR duplicates and technical duplicates which inflate our sequencing depth and give us false certainty in the genotype calls.

    04_markdup.sh
    
    #!/usr/bin/env bash

    java -jar /home/finn/picard-tools-1.119/MarkDuplicates.jar \
     REMOVE_DUPLICATES=true \
     ASSUME_SORTED=true VALIDATION_STRINGENCY=SILENT \
     MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000 \
     INPUT=/SAN/Ctyzzeri/all/results/align/*.bam \
     OUTPUT=/SAN/Ctyzzeri/all/results/rmd/*.rmd.bam \
     METRICS_FILE=/SAN/Ctyzzeri/all/results/qc/rmd/*.rmd.bam.metrics


### Indexing the BAM files

    05_samtools_index.sh
    
    #!/usr/bin/env bash
    
    samtools index /SAN/Ctyzzeri/all/results/rmd/*.bam

## Variant calling with bcftools


To call variants, we will first use the samtools mpileup tool to pileup our BAM files. What does this mean exactly? Well we will take all reads at a given position and call variants from the reads covering that position. We need to do this for all individuals. After performing the pileup, we than pass the output to bcftools call which will actually call variants. 

For bcftools mpileup:

-a - Annotate the vcf - here we add allelic depth (AD), genotype depth (DP) and strand bias (SP).

-O - the output type. Here it is u which means we do not compress the output.

-f - specify the reference genome to call variants against.


For bcftools call:

-f - format fields for the vcf - here they are genotype quality (GQ) and genotype probability (GP).

-v - output variant sites only - i.e. ignore non-variant parts of the reads

-m- use bcftools multiallelic caller

-O- specify the output type, here it is z - i.e. gzipped (compressed) vcf

-o output path

    06_mpileup.sh

    #!/usr/bin/env bash

    REF=/SAN/Ctyzzeri/all/resources/CryptoDB-57_CtyzzeriUGA55_Genome.fasta
    OUT=/SAN/Ctyzzeri/all/results/vcf/variants.vcf.gz

    bcftools mpileup -a AD,DP,SP -Ou -f $REF \
    /SAN/Ctyzzeri/all/results/rmd/*.bam | \
    bcftools call -f GQ,GP -m -O z -v --ploidy 1 -o $OUT
    
    

## indexing the vcf file

VCF stands for ‘variant call format’ and is a standard format used for variant calling and in population genomics. Again a detailed specification can be found online. It can take a bit of getting used to but it is widely supported and very useful. 

These are the first 11 fields of the vcf and they are always present. What do they mean?

- CHROM - chromosome or scaffold id from the reference genome
- POS - base pair reference position
- ID - SNP id - blank in this case
- REF- Reference base - A,C,G,T or N. More than one indicates an indel
- ALT - Alternaate base - the alternate base called as a variant
- QUAL - Phred quality score for alternate base call (site not individual)
- FILTER - filter status - if PASS then all filters passed
- INFO - additional info on each site - explanation stored in header
- FORMAT- the format for the genotype fields

![VCF format](https://raw.githubusercontent.com/tlobnow/Crypto/main/Genome_Analysis/markdown/vcf_format.png)

    07_vcfIndex.sh
    bcftools index /SAN/Ctyzzeri/all/results/vcf/variants.vcf.gz


## Generating statistics on the VCF with vcftools (local)

In order to generate statistics from our VCF and also actually later apply filters, we are going to use vcftools, a very useful and fast program for handling vcf files.

Determining how to set filters on a dataset is a bit of a nightmare - it is something newcomers (and actually experienced people too) really struggle with. There also isn’t always a clear answer - if you can justify your actions then there are often multiple solutions to how you set your filters. What is important is that you get to know your data and from that determine some sensible thresholds.

Luckily, vcftools makes it possible to easily calculate these statistics. In this section, we will analyse our VCF in order to get a sensible idea of how to set such filtering thresholds. The main areas we will consider are:

#### Depth: 

You should always include a minimum depth filter and ideally also a maximum depth one too. Minimum depth cutoffs will remove false positive calls and will ensure higher quality calls too. A maximum cut off is important because regions with very, very high read depths are likely repetitive ones mapping to multiple parts of the genome.

#### Quality:

Genotype quality is also an important filter - essentially you should not trust any genotype with a Phred score below 20 which suggests a less than 99% accuracy.

#### Minor allele frequency:

MAF can cause big problems with SNP calls - and also inflate statistical estimates downstream. Ideally you want an idea of the distribution of your allelic frequencies but 0.05 to 0.10 is a reasonable cut-off. You should keep in mind however that some analyses, particularly demographic inference can be biased by MAF thresholds.

#### Missing data:

How much missing data are you willing to tolerate? It will depend on the study but typically any site with >25% missing data should be dropped.

For bigger files, it makes sense to generate a vcfrandomsample subset, but these files are small enough.


The entire script to generate statistics can be looped as follows:

    08_vcfStats.sh

    INDS=($(for i in ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/*.vcf.gz; do echo $(basename -a -s *.vcf.gz $i); done))

    for IND in ${INDS[@]};
    do

        #### Calculate allele frequency
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --freq2 --out ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

        #### Calculate mean depth of coverage per individual
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --depth --out ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

        #### Calculate mean depth of coverage for  each site
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --site-mean-depth --out ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

        #### Calculate the site quality score for each site
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --site-quality --out ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

        #### Calculate the proportion of missing data per individual
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --missing-indv --out ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

        #### Calculate the propoortion of missing data per site
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --missing-site --out ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

        #### Calculate heterozygosity and inbreeding coefficient per individual
        vcftools --gzvcf ~/Documents/Github/Crypto/Genome_Analysis/products/vcf/${IND} --het --out      ~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/${IND}

    done

    

## Examining statistics

### Variant based statistics

The first thing we will do is look at the statistics we generated for each of the variants in our subset VCF - quality, depth, missingness and allele frequency.

#### Variant Quality

The first metric we will look at is the (Phred encoded) site quality. This is a measure of how much confidence we have in our variant calls. You will see that for each site in our subsampled VCF, we have extracted the site quality score.
Remember that a Phred score of 30 represents a 1 in 1000 chance that our SNP call is erroneous. Most sites should exceed this to suggest that we have a lot of high confidence calls. A minimum threshold of 30 should be set and other aspects of the data should be filtered more strongly.

``` {r stat-prep, echo = F, warning = F, message = F}
var_qual_04 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.lqual", col.names = c("chr", "pos", "qual"), skip = 1)
var_qual_05 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.lqual", col.names = c("chr", "pos", "qual"), skip = 1)
var_qual_06 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.lqual", col.names = c("chr", "pos", "qual"), skip = 1)
var_qual_07 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.lqual", col.names = c("chr", "pos", "qual"), skip = 1)
var_qual_12 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.lqual", col.names = c("chr", "pos", "qual"), skip = 1)

a <- ggplot(var_qual_04, aes(qual)) + geom_density(fill = "lightblue",      colour = "black", alpha = 0.3) + theme_light()
b <- ggplot(var_qual_05, aes(qual)) + geom_density(fill = "cornflowerblue", colour = "black", alpha = 0.3) + theme_light()
c <- ggplot(var_qual_06, aes(qual)) + geom_density(fill = "orange",         colour = "black", alpha = 0.3) + theme_light()
d <- ggplot(var_qual_07, aes(qual)) + geom_density(fill = "yellow",         colour = "black", alpha = 0.3) + theme_light()
e <- ggplot(var_qual_12, aes(qual)) + geom_density(fill = "green",          colour = "black", alpha = 0.3) + theme_light()

fig1 <- ggarrange(a,b,c,d,e, ncol = 3, nrow = 2, 
                    labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig1
```


#### Variant Mean Depth

Next we will examine the mean depth for each of our variants. This is essentially the number of reads that have mapped to this position. The output we generated with vcftools is the mean of the read depth across all individuals - it is for both alleles at a position and is not partitioned between the reference and the alternative.

Variant Depth with xlim(0,150) and ylim(0, 0.025) gives a better idea of the distribution. 

We could set our minimum coverage at the 5 and 95% quantiles but we should keep in mind that the more reads that cover a site, the higher confidence our basecall is. 10x is a good rule of thumb as a minimum cutoff for read depth, although if we wanted to be conservative, we could go with 15x.

What is more important here is that we set a good maximum depth cufoff. As the outliers show, some regions clearly have extremely high coverage and this likely reflects mapping/assembly errors and also paralogous or repetitive regions. We want to exclude these as they will bias our analyses. Usually a good rule of thumb is something the mean depth x 2 

``` {r variant-mean-depth, echo = F, warning = F, message = F}
var_depth_04 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.ldepth.mean", col.names = c("chr", "pos", "mean_depth", "var_depth"), skip = 1)
var_depth_05 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.ldepth.mean", col.names = c("chr", "pos", "mean_depth", "var_depth"), skip = 1)
var_depth_06 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.ldepth.mean", col.names = c("chr", "pos", "mean_depth", "var_depth"), skip = 1)
var_depth_07 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.ldepth.mean", col.names = c("chr", "pos", "mean_depth", "var_depth"), skip = 1)
var_depth_12 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.ldepth.mean", col.names = c("chr", "pos", "mean_depth", "var_depth"), skip = 1)

fig2 <- ggplot(show.legend = T) +
  geom_density(data = var_depth_04, aes(mean_depth, fill = "04_samples"), colour = "black", alpha = 0.3, show.legend = T) +
  geom_density(data = var_depth_05, aes(mean_depth, fill = "05_samples"), colour = "black", alpha = 0.3, show.legend = T) +
  geom_density(data = var_depth_06, aes(mean_depth, fill = "06_samples"), colour = "black", alpha = 0.3, show.legend = T) +
  geom_density(data = var_depth_07, aes(mean_depth, fill = "07_samples"), colour = "black", alpha = 0.3, show.legend = T) +
  geom_density(data = var_depth_12, aes(mean_depth, fill = "12_samples"), colour = "black", alpha = 0.3, show.legend = T) +
  theme_light() +
  scale_fill_manual(name="Legend",values=c('04_samples'="lightblue", 
                                           '05_samples'="cornflowerblue", 
                                           '06_samples'="orange", 
                                           '07_samples'="yellow", 
                                           '12_samples'="green")) +
  xlim(0,150) +
  ylim(0,0.025)
fig2
```


``` {r var-depth-summary, echo = F, warning = F, include = F, message = F}
# var_depth_04
summary(var_depth_04$mean_depth)
# var_depth_05
summary(var_depth_05$mean_depth)
# var_depth_06
summary(var_depth_06$mean_depth)
# var_depth_07
summary(var_depth_07$mean_depth)
# var_depth_12
summary(var_depth_12$mean_depth)
```


#### Variant Missingness

Next up we will look at the proportion of missingness at each variant. This is a measure of how many individuals lack a genotype at a call site.

The results indicate relatively high missingness, especially among _C.tyzzeri_ samples. One thing to note here is that vcftools inverts the direction of missigness, so our 10% threshold means we will tolerate 90% missingness. Typically missingness of 75-95% is used and we will need to accept 75% due to our missingness.

``` {r variant-missingness, echo = F, warning = F, include = F, message = F}
var_miss_04 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.lmiss", col.names = c("chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"), skip = 1)
var_miss_05 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.lmiss", col.names = c("chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"), skip = 1)
var_miss_06 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.lmiss", col.names = c("chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"), skip = 1)
var_miss_07 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.lmiss", col.names = c("chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"), skip = 1)
var_miss_12 <- read.delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.lmiss", col.names = c("chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"), skip = 1)

g <- ggplot(var_miss_04, aes(fmiss)) + geom_density(fill = "lightblue",      colour = "black", alpha = 0.3) + theme_light()
h <- ggplot(var_miss_05, aes(fmiss)) + geom_density(fill = "cornflowerblue", colour = "black", alpha = 0.3) + theme_light()
i <- ggplot(var_miss_06, aes(fmiss)) + geom_density(fill = "orange",         colour = "black", alpha = 0.3) + theme_light()
j <- ggplot(var_miss_07, aes(fmiss)) + geom_density(fill = "yellow",         colour = "black", alpha = 0.3) + theme_light()
k <- ggplot(var_miss_12, aes(fmiss)) + geom_density(fill = "green",          colour = "black", alpha = 0.3) + theme_light()

fig3 <- ggarrange(g,h,i,j,k, ncol = 3, nrow = 2, 
                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig3
```

``` {r var-miss-summary, echo = F, warning = F, message = F}
# var_miss_04
summary(var_miss_04$fmiss)
# var_depth_05
summary(var_miss_05$fmiss)
# var_miss_06
summary(var_miss_06$fmiss)
# var_miss_07
summary(var_miss_07$fmiss)
# var_miss_12
summary(var_miss_12$fmiss)
```

#### Minor Allele Frequency

Last of all for our per variant analyses, we will take a look at the distribution of allele frequencies. This will help inform our minor-allele frequency (MAF) thresholds.

We use apply on our allele frequencies to return the lowest allele frequency at each variant. We then added these to our dataframe as the variable maf.

``` {r, MAF, echo = F, warning = F, message = F}
# load data
var_freq_04 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.frq", col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2"), skip = 1)
var_freq_05 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.frq", col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2"), skip = 1)
var_freq_06 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.frq", col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2"), skip = 1)
var_freq_07 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.frq", col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2"), skip = 1)
var_freq_12 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.frq", col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2"), skip = 1)

# change type of a2 from chr to num
var_freq_04$a2 <- as.numeric(var_freq_04$a2)
var_freq_05$a2 <- as.numeric(var_freq_05$a2)
var_freq_06$a2 <- as.numeric(var_freq_06$a2)
var_freq_07$a2 <- as.numeric(var_freq_07$a2)
var_freq_12$a2 <- as.numeric(var_freq_12$a2)

# find minor allele frequency (maf)
var_freq_04$maf <- var_freq_04 %>% dplyr::select(a1, a2) %>% apply(1, function(z) min(z))
var_freq_05$maf <- var_freq_05 %>% dplyr::select(a1, a2) %>% apply(1, function(z) min(z))
var_freq_06$maf <- var_freq_06 %>% dplyr::select(a1, a2) %>% apply(1, function(z) min(z))
var_freq_07$maf <- var_freq_07 %>% dplyr::select(a1, a2) %>% apply(1, function(z) min(z))
var_freq_12$maf <- var_freq_12 %>% dplyr::select(a1, a2) %>% apply(1, function(z) min(z))

l <- ggplot(var_freq_04, aes(maf)) + geom_density(fill = "lightblue",      colour = "black", alpha = 0.3) + theme_light()
m <- ggplot(var_freq_05, aes(maf)) + geom_density(fill = "cornflowerblue", colour = "black", alpha = 0.3) + theme_light()
n <- ggplot(var_freq_06, aes(maf)) + geom_density(fill = "orange",         colour = "black", alpha = 0.3) + theme_light()
o <- ggplot(var_freq_07, aes(maf)) + geom_density(fill = "yellow",         colour = "black", alpha = 0.3) + theme_light()
p <- ggplot(var_freq_12, aes(maf)) + geom_density(fill = "green",          colour = "black", alpha = 0.3) + theme_light()

fig4 <- ggarrange(l,m,n,o,p, ncol = 3, nrow = 2, 
                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig4
```

``` {r, maf-summary, echo = F, warning = F, message = F}
summary(var_freq_04$maf)
summary(var_freq_05$maf)
summary(var_freq_06$maf)
summary(var_freq_07$maf)
summary(var_freq_12$maf)
```

The upper bound of the distribution is 0.5, which makes sense because if MAF was more than this, it wouldn’t be the MAF! How do we interpret MAF? It is an important measure because low MAF alleles may only occur in one or two individuals. It is possible that some of these low frequency alleles are in fact unreliable base calls - i.e. a source of error.

Setting MAF cutoffs is actually not that easy or straightforward. Hard MAF filtering (i.e. setting a high threshold) can severely bias estimation of the site frequency spectrum and cause problems with demographic analyses. Similarly, an excess of low frequency, ‘singleton’ SNPs (i.e. only occurring in one individual) can mean you keep many uninformative loci in your data set that make it hard to model things like population structure.

Usually then, it is best practice to produce one data set with a good MAF threshold and keep another without any MAF filtering at all.


### Individual based statistics

As well as a our per variant statistics we generated earlier, we also calculated some individual metrics too. WE can look at the distribution of these to get an idea whether some of our individuals have not sequenced or mapped as well as others. This is good practice to do with a new dataset. A lot of these statistics can be compared to other measures generated from the data (i.e. principal components as a measure of population structure) to see if they drive any apparent patterns in the data.

#### Mean depth per individual

``` {r, mean-depth-ind, echo = F, warning = F, message = F}
# load data
ind_depth_04 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.idepth", col_names = c("ind", "nsites", "depth"), skip = 1)
ind_depth_05 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.idepth", col_names = c("ind", "nsites", "depth"), skip = 1)
ind_depth_06 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.idepth", col_names = c("ind", "nsites", "depth"), skip = 1)
ind_depth_07 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.idepth", col_names = c("ind", "nsites", "depth"), skip = 1)
ind_depth_12 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.idepth", col_names = c("ind", "nsites", "depth"), skip = 1)

q <- ggplot(ind_depth_04, aes(depth)) + geom_histogram(fill = "lightblue",      colour = "black", alpha = 0.3) + theme_light()
r <- ggplot(ind_depth_05, aes(depth)) + geom_histogram(fill = "cornflowerblue", colour = "black", alpha = 0.3) + theme_light()
s <- ggplot(ind_depth_06, aes(depth)) + geom_histogram(fill = "orange",         colour = "black", alpha = 0.3) + theme_light()
t <- ggplot(ind_depth_07, aes(depth)) + geom_histogram(fill = "yellow",         colour = "black", alpha = 0.3) + theme_light()
u <- ggplot(ind_depth_12, aes(depth)) + geom_histogram(fill = "green",          colour = "black", alpha = 0.3) + theme_light()

fig5 <- ggarrange(q,r,s,t,u, ncol = 3, nrow = 2, 
                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig5
```

Because we are only plotting data for few individuals, the plot looks a little disjointed. While there is some evidence that some individuals were sequenced to a higher depth than others, there are no extreme outliers. So this doesn’t suggest any issue with individual sequencing depth.


#### Proportion of missing data per individual

Next we will look at the proportion of missing data per individual. This is very similar to the missing data per site, but here we will focus on the fmiss column - i.e. the proportion of missing data.

``` {r, miss-ind, echo = F, warning = F, message = F}
# load data
ind_miss_04 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.imiss", col_names = c("ind", "ndata", "nfiltered", "nmiss", "fmiss"), skip = 1)
ind_miss_05 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.imiss", col_names = c("ind", "ndata", "nfiltered", "nmiss", "fmiss"), skip = 1)
ind_miss_06 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.imiss", col_names = c("ind", "ndata", "nfiltered", "nmiss", "fmiss"), skip = 1)
ind_miss_07 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.imiss", col_names = c("ind", "ndata", "nfiltered", "nmiss", "fmiss"), skip = 1)
ind_miss_12 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.imiss", col_names = c("ind", "ndata", "nfiltered", "nmiss", "fmiss"), skip = 1)

v <- ggplot(ind_miss_04, aes(fmiss)) + geom_histogram(fill = "lightblue",      colour = "black", alpha = 0.3) + theme_light()
w <- ggplot(ind_miss_05, aes(fmiss)) + geom_histogram(fill = "cornflowerblue", colour = "black", alpha = 0.3) + theme_light()
x <- ggplot(ind_miss_06, aes(fmiss)) + geom_histogram(fill = "orange",         colour = "black", alpha = 0.3) + theme_light()
y <- ggplot(ind_miss_07, aes(fmiss)) + geom_histogram(fill = "yellow",         colour = "black", alpha = 0.3) + theme_light()
z <- ggplot(ind_miss_12, aes(fmiss)) + geom_histogram(fill = "green",          colour = "black", alpha = 0.3) + theme_light()

fig6 <- ggarrange(v,w,x,y,z, ncol = 3, nrow = 2, 
                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig6
```
Again this shows us, the proportion of missing data per individual is very small indeed. It ranges from 0.01 - 0.6, so we can safely assume that some of our individuals are sequenced well, while the extreme values at 0.6 indicate rather bad sequencing that probably arises from one of the _C.tyzzeri_ samples (AA_0900 I would assume due to reportedly bad sequencing). We will therefore set a threshold at 0.1.

#### Heterozygosity and inbreeding coefficient per individual

We cannot assess heterozygosity due to the haploid nature of _Cryptosporidium_ spp.

Technically, computing heterozygosity and the inbreeding coefficient (F) for each individual can quickly highlight outlier individuals that are e.g. inbred (strongly negative F), suffer from high sequencing error problems or contamination with DNA from another individual leading to inflated heterozygosity (high F), or PCR duplicates or low read depth leading to allelic dropout and thus underestimated heterozygosity (stongly negative F). However, note that here we assume Hardy-Weinberg equilibrium. If the individuals are not sampled from the same population, the expected heterozygosity will be overestimated due to the Wahlund-effect. It may still be worth to compute heterozygosities even if the samples are from more than one population to check if any of the individuals stands out which could indicate problems.

``` {r, het-ind, echo = F, warning = F, include = F, message = F}
# load data
ind_het_04 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/04_samples.vcf.gz.het", col_names = c("ind","ho", "he", "nsites", "f"), skip = 1)
ind_het_05 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/05_samples.vcf.gz.het", col_names = c("ind","ho", "he", "nsites", "f"), skip = 1)
ind_het_06 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/06_samples.vcf.gz.het", col_names = c("ind","ho", "he", "nsites", "f"), skip = 1)
ind_het_07 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/07_samples.vcf.gz.het", col_names = c("ind","ho", "he", "nsites", "f"), skip = 1)
ind_het_12 <- read_delim("~/Documents/Github/Crypto/Genome_Analysis/products/vcftools/12_samples.vcf.gz.het", col_names = c("ind","ho", "he", "nsites", "f"), skip = 1)

a <- ggplot(ind_het_04, aes(f)) + geom_histogram(fill = "lightblue",      colour = "black", alpha = 0.3) + theme_light()
b <- ggplot(ind_het_05, aes(f)) + geom_histogram(fill = "cornflowerblue", colour = "black", alpha = 0.3) + theme_light()
c <- ggplot(ind_het_06, aes(f)) + geom_histogram(fill = "orange",         colour = "black", alpha = 0.3) + theme_light()
d <- ggplot(ind_het_07, aes(f)) + geom_histogram(fill = "yellow",         colour = "black", alpha = 0.3) + theme_light()
e <- ggplot(ind_het_12, aes(f)) + geom_histogram(fill = "green",          colour = "black", alpha = 0.3) + theme_light()

fig6 <- ggarrange(a,b,c,d,e, ncol = 3, nrow = 2, 
                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig6
```


## Filtering the VCF with vcftools (local)

Now we have an idea of how to set out thresholds, we will do just that. 

#### attributes for filtering:

- --gvcf - input path – denotes a gzipped vcf file
- --remove-indels - remove all indels (SNPs only)
- --maf - set minor allele frequency - 0.1 here
- --max-missing - set minimum missing data. A little counterintuitive - 0 is totally missing, 1 is none - missing. Here 0.75 means we will tolerate 25% missing data.
- --minQ - this is just the minimum quality score required for a site to pass our filtering threshold. Here we set it to 30.
- --min-meanDP - the minimum mean depth for a site.
- --max-meanDP - the maximum mean depth for a site.
- --minDP - the minimum depth allowed for a genotype - any individual failing this threshold is marked as having a missing genotype.
- --maxDP - the maximum depth allowed for a genotype - any individual failing this threshold is marked as having a missing genotype.
- --recode - recode the output - necessary to output a vcf
- --stdout - pipe the vcf out to the stdout (easier for file handling)

(vcftools is NOT available on Harriet..)

    09_filterVCF.sh
    
    #!/usr/bin/env bash

    VCF_IN=~/Desktop/vcf/variants.vcf
    VCF_OUT=~/Desktop/vcfFiltered/variants_filtered.vcf.gz

    # set filters
    MAF=0.05
    MISS=0.75
    QUAL=30
    MIN_DEPTH=10
    MAX_DEPTH=50

    # perform the filtering with vcftools
    vcftools --gzvcf $VCF_IN \
    --remove-indels --maf $MAF --max-missing $MISS --minQ $QUAL \
    --min-meanDP $MIN_DEPTH --max-meanDP $MAX_DEPTH \
    --minDP $MIN_DEPTH --maxDP $MAX_DEPTH --recode --stdout | gzip -c > \
    $VCF_OUT


## Linkage Pruning

One of the major assumptions of PCA is that the data we use is independent - i.e. there are no spurious correlations among the measured variables. This is obviously not the case for most genomic data as allele frequencies are correlated due to physical linkage and linkage disequilibrium. So as a first step, we need to prune our data set of variants that are in linkage.

- --vcf - specified the location of our VCF file.

- --double-id - told plink to duplicate the id of our samples (this is because plink typically expects a family and individual id - i.e. for pedigree data - this is not necessary for us.

- --allow-extra-chr - allow additional chromosomes beyond the human chromosome set. This is necessary as otherwise plink expects chromosomes 1-22 and the human X chromosome.

- --set-missing-var-ids - also necessary to set a variant ID for our SNPs. Human and model organisms often have annotated SNP names and so plink will look for these. We do not have them so instead we set ours to default to chromosome:position which can be achieved in plink by setting the option @:# - see here for more info.

- --indep-pairwise - finally we are actually on the command that performs our linkage pruning! The first argument, 50 denotes we have set a window of 50 Kb. The second argument, 10 is our window step size - meaning we move 10 bp each time we calculate linkage. Finally, we set an r2 threshold - i.e. the threshold of linkage we are willing to tolerate. Here we prune any variables that show an r2 of greater than 0.1.

- --out Produce the prefix for the output data.

    10_linkagePruning.sh
    
    VCF=/SAN/Ctyzzeri/all/results/vcfFiltered/*_samples_filtered.vcf.gz

    ~/plink-1.9/plink \
    --vcf $VCF --double-id --allow-extra-chr \
    --set-missing-var-ids @:#\$1,\$2 \
    --indep-pairwise 50 10 0.1 --out *_samples


## Principal Component Analysis (PCA)

We can rerun plink with a few additional arguments to get it to conduct a PCA.

- --extract - this just lets plink know we want to extract only these positions from our VCF - in other words, the analysis will only be conducted on these.

- --make-bed - this is necessary to write out some additional files for another type of population structure analysis - a model based approach with admixture.

- --pca - fairly self explanatory, this tells plink to calculate a principal components analysis.

PCA output:

- cichlids.eigenval - the eigenvalues from our analysis

- cichlids.eigenvec- the eigenvectors from our analysis

Plink binary output:

- cichlids.bed - the cichlids bed file - this is a binary file necessary for admixture analysis. It is essentially the genotypes of the pruned dataset recoded as 1s and 0s.

- cichlids.bim - a map file (i.e. information file) of the variants contained in the bed file.

- cichlids.fam - a map file for the individuals contained in the bed file.

    08_pca.sh
    
    #!/usr/bin/env bash

    INDS=($(for i in /SAN/Ctyzzeri/all/results/vcfFiltered/*_filtered.vcf.gz; do echo $(basename -a -s _filtered.vcf.gz $i); done))

    for IND in ${INDS[@]};
    do

        ~/plink-1.9/plink \
        --vcf /SAN/Ctyzzeri/all/results/vcfFiltered/${IND}_filtered.vcf.gz --double-id --allow-extra-chr \
        --set-missing-var-ids @:#\$1,\$2 \
        --extract /SAN/Ctyzzeri/all/results/pcaFiltered/${IND}.prune.in  \
        --make-bed --pca --out /SAN/Ctyzzeri/all/results/pcaFiltered/${IND}

    done


# PCA ANALYSIS


## Bar plot 
First we will plot the eigenvalues. It is quite straightforward to translate these into percentage variance explained (although note, you could just plot these raw if you wished). Let's take a look at the the bar plot showing the percentage of variance each principal component explains.

``` {r, bar-04-samples, echo = F, warning = F, message = F}
# LOAD THE PCA DATA
pca_04 <- read_table("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/04_samples.eigenvec", col_names = F)
eigenval_04 <- scan("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/04_samples.eigenval")

pca_05 <- read_table("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/05_samples.eigenvec", col_names = F)
eigenval_05 <- scan("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/05_samples.eigenval")

pca_06 <- read_table("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/06_samples.eigenvec", col_names = F)
eigenval_06 <- scan("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/06_samples.eigenval")

pca_07 <- read_table("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/07_samples.eigenvec", col_names = F)
eigenval_07 <- scan("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/07_samples.eigenval")

pca_12 <- read_table("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/12_samples.eigenvec", col_names = F)
eigenval_12 <- scan("~/Documents/Github/Crypto/Genome_Analysis/products/pcaFiltered/12_samples.eigenval")

# CLEANING
# First remove a nuisance col (plink outputs the individual ID twice)
# Then we will give the pca data.frame proper colnames (PC1, PC2, ...)
pca_04 <- pca_04[,-1]
pca_05 <- pca_05[,-1]
pca_06 <- pca_06[,-1]
pca_07 <- pca_07[,-1]
pca_12 <- pca_12[,-1]
names(pca_04)[1] <- "ind"
names(pca_05)[1] <- "ind"
names(pca_06)[1] <- "ind"
names(pca_07)[1] <- "ind"
names(pca_12)[1] <- "ind"
names(pca_04)[2:ncol(pca_04)] <- paste0("PC", 1:(ncol(pca_04)-1))
names(pca_05)[2:ncol(pca_05)] <- paste0("PC", 1:(ncol(pca_05)-1))
names(pca_06)[2:ncol(pca_06)] <- paste0("PC", 1:(ncol(pca_06)-1))
names(pca_07)[2:ncol(pca_07)] <- paste0("PC", 1:(ncol(pca_07)-1))
names(pca_12)[2:ncol(pca_12)] <- paste0("PC", 1:(ncol(pca_12)-1))


# We add a host species, location, and species x location vector
# sample
sample_04 <- rep(NA, length(pca_04$ind))
sample_05 <- rep(NA, length(pca_05$ind))
sample_06 <- rep(NA, length(pca_06$ind))
sample_07 <- rep(NA, length(pca_07$ind))
sample_12 <- rep(NA, length(pca_12$ind))

sample_04[grep("866",   pca_04$ind)] <- "866"
sample_04[grep("942",   pca_04$ind)] <- "942"
sample_04[grep("900",   pca_04$ind)] <- "900"
sample_04[grep("UGA55", pca_04$ind)] <- "UGA55"

sample_05[grep("866",   pca_05$ind)] <- "866"
sample_05[grep("942",   pca_05$ind)] <- "942"
sample_05[grep("900",   pca_05$ind)] <- "900"
sample_05[grep("TYGZ1", pca_05$ind)] <- "TYGZ1"
sample_05[grep("UGA55", pca_05$ind)] <- "UGA55"

sample_06[grep("866",   pca_06$ind)] <- "866"
sample_06[grep("942",   pca_06$ind)] <- "942"
sample_06[grep("900",   pca_06$ind)] <- "900"
sample_06[grep("TYGZ1", pca_06$ind)] <- "TYGZ1"
sample_06[grep("UGA55", pca_06$ind)] <- "UGA55"
sample_06[grep("PAR", pca_06$ind)] <- "PAR"

sample_07[grep("866",   pca_07$ind)] <- "866"
sample_07[grep("942",   pca_07$ind)] <- "942"
sample_07[grep("900",   pca_07$ind)] <- "900"
sample_07[grep("TYGZ1", pca_07$ind)] <- "TYGZ1"
sample_07[grep("UGA55", pca_07$ind)] <- "UGA55"
sample_07[grep("PAR", pca_07$ind)] <- "PAR"
sample_07[grep("HOM", pca_07$ind)] <- "HOM"


sample_12[grep("866",   pca_12$ind)] <- "866"
sample_12[grep("942",   pca_12$ind)] <- "942"
sample_12[grep("900",   pca_12$ind)] <- "900"
sample_12[grep("TYGZ1", pca_12$ind)] <- "TYGZ1"
sample_12[grep("UGA55", pca_12$ind)] <- "UGA55"
sample_12[grep("PAR", pca_12$ind)] <- "PAR"
sample_12[grep("HOM", pca_12$ind)] <- "HOM"
sample_12[grep("USA_P", pca_12$ind)] <- "USA_P"
sample_12[grep("FRA_P", pca_12$ind)] <- "FRA_P"
sample_12[grep("CHN_P", pca_12$ind)] <- "CHN_P"
sample_12[grep("CZE_P", pca_12$ind)] <- "CZE_P"
sample_12[grep("NZL_H", pca_12$ind)] <- "NZL_H"
sample_12[grep("GBR_H", pca_12$ind)] <- "GBR_H"
sample_12[grep("UGA_H", pca_12$ind)] <- "UGA_H"

# location
loc_04 <- rep(NA, length(pca_04$ind))
loc_05 <- rep(NA, length(pca_05$ind))
loc_06 <- rep(NA, length(pca_06$ind))
loc_07 <- rep(NA, length(pca_07$ind))
loc_12 <- rep(NA, length(pca_12$ind))

loc_04[grep("866",   pca_04$ind)] <- "GER"
loc_04[grep("942",   pca_04$ind)] <- "GER"
loc_04[grep("900",   pca_04$ind)] <- "GER"
loc_04[grep("UGA55", pca_04$ind)] <- "USA"

loc_05[grep("866",   pca_05$ind)] <- "GER"
loc_05[grep("942",   pca_05$ind)] <- "GER"
loc_05[grep("900",   pca_05$ind)] <- "GER"
loc_05[grep("TYGZ1", pca_05$ind)] <- "CHN"
loc_05[grep("UGA55", pca_05$ind)] <- "USA"

loc_06[grep("866",   pca_06$ind)] <- "GER"
loc_06[grep("942",   pca_06$ind)] <- "GER"
loc_06[grep("900",   pca_06$ind)] <- "GER"
loc_06[grep("TYGZ1", pca_06$ind)] <- "CHN"
loc_06[grep("UGA55", pca_06$ind)] <- "USA"
loc_06[grep("PAR", pca_06$ind)] <- "USA"

loc_07[grep("866",   pca_07$ind)] <- "GER"
loc_07[grep("942",   pca_07$ind)] <- "GER"
loc_07[grep("900",   pca_07$ind)] <- "GER"
loc_07[grep("TYGZ1", pca_07$ind)] <- "CHN"
loc_07[grep("UGA55", pca_07$ind)] <- "USA"
loc_07[grep("PAR", pca_07$ind)] <- "USA"
loc_07[grep("HOM", pca_07$ind)] <- "NZ"


loc_12[grep("866",   pca_12$ind)] <- "GER"
loc_12[grep("942",   pca_12$ind)] <- "GER"
loc_12[grep("900",   pca_12$ind)] <- "GER"
loc_12[grep("TYGZ1", pca_12$ind)] <- "CHN"
loc_12[grep("UGA55", pca_12$ind)] <- "USA"
loc_12[grep("PAR", pca_12$ind)] <- "USA"
loc_12[grep("HOM", pca_12$ind)] <- "NZ"
loc_12[grep("USA_P", pca_12$ind)] <- "USA"
loc_12[grep("FRA_P", pca_12$ind)] <- "FRA"
loc_12[grep("CHN_P", pca_12$ind)] <- "CHN"
loc_12[grep("CZE_P", pca_12$ind)] <- "CZE"
loc_12[grep("NZL_H", pca_12$ind)] <- "NZL"
loc_12[grep("GBR_H", pca_12$ind)] <- "GBR"
loc_12[grep("UGA_H", pca_12$ind)] <- "UGA"

# host species
host_04 <- rep(NA, length(pca_04$ind))
host_05 <- rep(NA, length(pca_05$ind))
host_06 <- rep(NA, length(pca_06$ind))
host_07 <- rep(NA, length(pca_07$ind))
host_12 <- rep(NA, length(pca_12$ind))

host_04[grep("866",   pca_04$ind)] <- "Mmd"
host_04[grep("942",   pca_04$ind)] <- "Mmd"
host_04[grep("900",   pca_04$ind)] <- "Mmm"
host_04[grep("UGA55", pca_04$ind)] <- "Mmd"

host_05[grep("866",   pca_05$ind)] <- "Mmd"
host_05[grep("942",   pca_05$ind)] <- "Mmd"
host_05[grep("900",   pca_05$ind)] <- "Mmm"
host_05[grep("TYGZ1", pca_05$ind)] <- "Mmm"
host_05[grep("UGA55", pca_05$ind)] <- "Mmd"

host_06[grep("866",   pca_06$ind)] <- "Mmd"
host_06[grep("942",   pca_06$ind)] <- "Mmd"
host_06[grep("900",   pca_06$ind)] <- "Mmm"
host_06[grep("TYGZ1", pca_06$ind)] <- "Mmm"
host_06[grep("UGA55", pca_06$ind)] <- "Mmd"
host_06[grep("PAR", pca_06$ind)] <- "Cattle"

host_07[grep("866",   pca_07$ind)] <- "Mmd"
host_07[grep("942",   pca_07$ind)] <- "Mmd"
host_07[grep("900",   pca_07$ind)] <- "Mmm"
host_07[grep("TYGZ1", pca_07$ind)] <- "Mmm"
host_07[grep("UGA55", pca_07$ind)] <- "Mmd"
host_07[grep("PAR", pca_07$ind)] <- "Cattle"
host_07[grep("HOM", pca_07$ind)] <- "Human"


host_12[grep("866",   pca_12$ind)] <- "Mmd"
host_12[grep("942",   pca_12$ind)] <- "Mmd"
host_12[grep("900",   pca_12$ind)] <- "Mmm"
host_12[grep("TYGZ1", pca_12$ind)] <- "Mmm"
host_12[grep("UGA55", pca_12$ind)] <- "Mmd"
host_12[grep("PAR", pca_12$ind)] <- "Cattle"
host_12[grep("HOM", pca_12$ind)] <- "Human"
host_12[grep("USA_P", pca_12$ind)] <- "Cattle"
host_12[grep("FRA_P", pca_12$ind)] <- "Human"
host_12[grep("CHN_P", pca_12$ind)] <- "Cattle"
host_12[grep("CZE_P", pca_12$ind)] <- "Human"
host_12[grep("NZL_H", pca_12$ind)] <- "Human"
host_12[grep("GBR_H", pca_12$ind)] <- "Human"
host_12[grep("UGA_H", pca_12$ind)] <- "Human"

# parasite species
spp_04 <- rep(NA, length(pca_04$ind))
spp_05 <- rep(NA, length(pca_05$ind))
spp_06 <- rep(NA, length(pca_06$ind))
spp_07 <- rep(NA, length(pca_07$ind))
spp_12 <- rep(NA, length(pca_12$ind))

spp_04[grep("866",   pca_04$ind)] <- "C.tyzzeri"
spp_04[grep("942",   pca_04$ind)] <- "C.tyzzeri"
spp_04[grep("900",   pca_04$ind)] <- "C.tyzzeri"
spp_04[grep("UGA55", pca_04$ind)] <- "C.tyzzeri"

spp_05[grep("866",   pca_05$ind)] <- "C.tyzzeri"
spp_05[grep("942",   pca_05$ind)] <- "C.tyzzeri"
spp_05[grep("900",   pca_05$ind)] <- "C.tyzzeri"
spp_05[grep("TYGZ1", pca_05$ind)] <- "C.tyzzeri"
spp_05[grep("UGA55", pca_05$ind)] <- "C.tyzzeri"

spp_06[grep("866",   pca_06$ind)] <- "C.tyzzeri"
spp_06[grep("942",   pca_06$ind)] <- "C.tyzzeri"
spp_06[grep("900",   pca_06$ind)] <- "C.tyzzeri"
spp_06[grep("TYGZ1", pca_06$ind)] <- "C.tyzzeri"
spp_06[grep("UGA55", pca_06$ind)] <- "C.tyzzeri"
spp_06[grep("PAR", pca_06$ind)] <- "C.parvum"

spp_07[grep("866",   pca_07$ind)] <- "C.tyzzeri"
spp_07[grep("942",   pca_07$ind)] <- "C.tyzzeri"
spp_07[grep("900",   pca_07$ind)] <- "C.tyzzeri"
spp_07[grep("TYGZ1", pca_07$ind)] <- "C.tyzzeri"
spp_07[grep("UGA55", pca_07$ind)] <- "C.tyzzeri"
spp_07[grep("PAR", pca_07$ind)] <- "C.parvum"
spp_07[grep("HOM", pca_07$ind)] <- "C.hominis"

spp_12[grep("866",   pca_12$ind)] <- "C.tyzzeri"
spp_12[grep("942",   pca_12$ind)] <- "C.tyzzeri"
spp_12[grep("900",   pca_12$ind)] <- "C.tyzzeri"
spp_12[grep("TYGZ1", pca_12$ind)] <- "C.tyzzeri"
spp_12[grep("UGA55", pca_12$ind)] <- "C.tyzzeri"
spp_12[grep("PAR", pca_12$ind)] <- "C.parvum"
spp_12[grep("HOM", pca_12$ind)] <- "C.hominis"
spp_12[grep("USA_P", pca_12$ind)] <- "C.parvum"
spp_12[grep("FRA_P", pca_12$ind)] <- "C.parvum"
spp_12[grep("CHN_P", pca_12$ind)] <- "C.parvum"
spp_12[grep("CZE_P", pca_12$ind)] <- "C.parvum"
spp_12[grep("NZL_H", pca_12$ind)] <- "C.hominis"
spp_12[grep("GBR_H", pca_12$ind)] <- "C.hominis"
spp_12[grep("UGA_H", pca_12$ind)] <- "C.hominis"


# remake data.frame with as.tibble (easier summaries etc.)
pca_04 <- as_tibble(data.frame(pca_04, sample_04, loc_04, host_04, spp_04))
pca_05 <- as_tibble(data.frame(pca_05, sample_05, loc_05, host_05, spp_05))
pca_06 <- as_tibble(data.frame(pca_06, sample_06, loc_06, host_06, spp_06))
pca_07 <- as_tibble(data.frame(pca_07, sample_07, loc_07, host_07, spp_07))
pca_12 <- as_tibble(data.frame(pca_12, sample_12, loc_12, host_12, spp_12))


# PLOTTING #####################################################################
## convert to percentage variance explained
pve_04 <- data.frame(PC = 1:length(pca_04$ind), pve = eigenval_04/sum(eigenval_04)*100)
pve_05 <- data.frame(PC = 1:length(pca_05$ind), pve = eigenval_05/sum(eigenval_05)*100)
pve_06 <- data.frame(PC = 1:length(pca_06$ind), pve = eigenval_06/sum(eigenval_06)*100)
pve_07 <- data.frame(PC = 1:length(pca_07$ind), pve = eigenval_07/sum(eigenval_07)*100)
pve_12 <- data.frame(PC = 1:length(pca_12$ind), pve = eigenval_12/sum(eigenval_12)*100)


# make plot
a <- pve_04 %>% ggplot(aes(PC, pve)) + geom_bar(stat = "identity") + ylab("Percentage variance explained") + theme_light()
b <- pve_05 %>% ggplot(aes(PC, pve)) + geom_bar(stat = "identity") + ylab("Percentage variance explained") + theme_light()
c <- pve_06 %>% ggplot(aes(PC, pve)) + geom_bar(stat = "identity") + ylab("Percentage variance explained") + theme_light()
d <- pve_07 %>% ggplot(aes(PC, pve)) + geom_bar(stat = "identity") + ylab("Percentage variance explained") + theme_light()
e <- pve_12 %>% ggplot(aes(PC, pve)) + geom_bar(stat = "identity") + ylab("Percentage variance explained") + theme_light()

fig7 <- ggarrange(a,b,c,d,e, ncol = 3, nrow = 2, 
                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
fig7

```

Cumulatively, they explain 100% of the variance but PC1, PC2 (and possible PC3) together explain most of the variance.

``` {r, cum, echo = F, warning = F, message = F}
# calculate the cumulative sum of the percentage variance explained
cumsum(pve_04$pve)
cumsum(pve_05$pve)
cumsum(pve_06$pve)
cumsum(pve_07$pve)
cumsum(pve_12$pve)
```

``` {r, PCA-04-samples, echo = F, warning = F, message = F}
# plot pca
f <- pca_04 %>% ggplot(aes((signif(pve_04$pve[1], 3)*PC1), 
                           (signif(pve_04$pve[2], 3))*PC2, col = spp_04, shape = host_04, label = sample_04)) + geom_point(size = 3) + 
  geom_label(nudge_y = 2) + scale_color_manual(values = c("red", "blue", "orange")) + coord_equal() + theme_light() + 
  xlab(paste0("PC1 (", signif(pve_04$pve[1], 3), "%)")) + ylab(paste0("PC2 (", signif(pve_04$pve[2], 3), "%)"))

g <- pca_05 %>% ggplot(aes((signif(pve_05$pve[1], 3)*PC1), 
                           (signif(pve_05$pve[2], 3))*PC2, col = spp_05, shape = host_05, label = sample_05)) + geom_point(size = 3) + 
  geom_label(nudge_y = 2) + scale_color_manual(values = c("red", "blue", "orange")) + coord_equal() + theme_light() + 
  xlab(paste0("PC1 (", signif(pve_05$pve[1], 3), "%)")) + ylab(paste0("PC2 (", signif(pve_05$pve[2], 3), "%)"))

h <- pca_06 %>% ggplot(aes((signif(pve_06$pve[1], 3)*PC1), 
                           (signif(pve_06$pve[2], 3))*PC2, col = spp_06, shape = host_06, label = sample_06)) + geom_point(size = 3) + 
  geom_label(nudge_y = 2) + scale_color_manual(values = c("blue", "red", "orange")) + coord_equal() + theme_light() + 
  xlab(paste0("PC1 (", signif(pve_06$pve[1], 3), "%)")) + ylab(paste0("PC2 (", signif(pve_06$pve[2], 3), "%)"))

i <- pca_07 %>% ggplot(aes((signif(pve_07$pve[1], 3)*PC1), 
                           (signif(pve_07$pve[2], 3))*PC2, col = spp_07, shape = host_07, label = sample_07)) + geom_point(size = 3) + 
  geom_label(nudge_y = 2) + scale_color_manual(values = c("orange", "blue", "red")) + coord_equal() + theme_light() + 
  xlab(paste0("PC1 (", signif(pve_07$pve[1], 3), "%)")) + ylab(paste0("PC2 (", signif(pve_07$pve[2], 3), "%)"))

j <- pca_12 %>% ggplot(aes((signif(pve_12$pve[1], 3)*PC1), 
                           (signif(pve_12$pve[2], 3))*PC2, col = spp_12, shape = host_12, label = sample_12)) + geom_point(size = 3) + 
  geom_label(nudge_y = 2) + scale_color_manual(values = c("orange", "blue", "red")) + coord_equal() + theme_light() + 
  xlab(paste0("PC1 (", signif(pve_12$pve[1], 3), "%)")) + ylab(paste0("PC2 (", signif(pve_12$pve[2], 3), "%)"))

f
g
h
i
j

#fig8 <- ggarrange(f,g,h,i,j, ncol = 1, nrow = 5, 
#                  labels = c('04_samples', '05_samples', '06_samples', '07_samples', '12_samples'))
#fig8

```





